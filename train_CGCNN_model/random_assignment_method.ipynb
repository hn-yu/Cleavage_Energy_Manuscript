{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import all packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import multiprocess as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import tqdm\n",
    "import torch\n",
    "import skorch.callbacks.base\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, 'adamwr') # you will need to have adamW optimizer cloned locally\n",
    "sys.path.insert(0, 'cgcnn/')\n",
    "import cgcnn\n",
    "import mongo\n",
    "\n",
    "from cgcnn.data import collate_pool, MergeDataset, StructureDataTransformer\n",
    "from cgcnn.model import CrystalGraphConvNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from skorch.callbacks import Checkpoint, LoadInitState \n",
    "from skorch.callbacks.lr_scheduler import WarmRestartLR, LRScheduler\n",
    "from skorch.dataset import ValidSplit\n",
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "# from cosine_scheduler import CosineLRWithRestarts\n",
    "# from adamw import AdamW\n",
    "from torch.optim import AdamW\n",
    "\n",
    "#Select which GPU to use if necessary\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the cleavage energy docs and convert the structures into graph objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pickle.load(open('../cleavage_energy_dataset/intermetallics_cleavage_energy_data.pkl' ,'rb'))\n",
    "random.seed(123)\n",
    "random.shuffle(docs)\n",
    "\n",
    "for doc in docs:\n",
    "    doc[\"atoms\"] = doc['thinnest_structure']['atoms']\n",
    "    doc[\"results\"] = doc['thinnest_structure']['results']\n",
    "    doc[\"initial_configuration\"] = doc['thinnest_structure']['initial_configuration']\n",
    "    del doc[\"thinnest_structure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3033/3033 [00:00<00:00, 2351011.65it/s]\n"
     ]
    }
   ],
   "source": [
    "SDT = StructureDataTransformer(atom_init_loc='C:\\\\Users\\\\1\\\\Documents\\\\GitHub\\\\Cleavage_Energy_Manuscript\\\\train_CGCNN_model\\\\cgcnn\\\\atom_init.json',\n",
    "                              max_num_nbr=12,\n",
    "                              step=0.8,\n",
    "                              radius=4,\n",
    "                              use_voronoi=False,\n",
    "                              use_tag=False,\n",
    "                              use_fixed_info=False,\n",
    "                              use_distance=False,\n",
    "                              train_geometry = 'initial'\n",
    "                              )\n",
    "\n",
    "SDT_out = SDT.transform(docs)\n",
    "structures = SDT_out[0]\n",
    "\n",
    "#Settings necessary to build the model (since they are size of vectors as inputs)\n",
    "orig_atom_fea_len = structures[0].shape[-1]\n",
    "nbr_fea_len = structures[1].shape[-1]\n",
    "def process_item(index, SDT_out):\n",
    "    return SDT_out[index]\n",
    "\n",
    "# Corrected multiprocessing usage\n",
    "with mp.Pool(4) as pool:\n",
    "    SDT_list = list(tqdm.tqdm(pool.starmap(process_item, [(i, SDT_out) for i in range(len(SDT_out))], chunksize=40), total=len(SDT_out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare prediction labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = np.array([[int(docs.index(doc)), np.log(doc['cleavage_energy'])] for doc in docs])\n",
    "target_list = pd.DataFrame(target_list, columns = ['doc_index', 'cleavage_energy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into 80:20 train:test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDT_training, SDT_test, target_training, target_test = train_test_split(SDT_list, target_list, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "#Make a checkpoint to save parameters every time there is a new best for validation lost\n",
    "cp = Checkpoint(monitor='valid_loss_best',fn_prefix='valid_best_')\n",
    "\n",
    "#Callback to load the checkpoint with the best validation loss at the end of training\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('valid_best_params.pt')\n",
    "        \n",
    "load_best_valid_loss = train_end_load_best_valid_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the model and train the model with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426\n",
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[[1.2317e-04, 1.8292e-02, 3.6764e-01, 1.0000e+00, 3.6812e-01,\n",
      "          1.8340e-02],\n",
      "         [2.8036e-05, 6.6921e-03, 2.1618e-01, 9.4511e-01, 5.5919e-01,\n",
      "          4.4776e-02],\n",
      "         [2.8036e-05, 6.6921e-03, 2.1618e-01, 9.4511e-01, 5.5919e-01,\n",
      "          4.4776e-02],\n",
      "         ...,\n",
      "         [4.8041e-08, 6.4998e-05, 1.1901e-02, 2.9491e-01, 9.8903e-01,\n",
      "          4.4889e-01],\n",
      "         [4.8041e-08, 6.4998e-05, 1.1901e-02, 2.9491e-01, 9.8903e-01,\n",
      "          4.4889e-01],\n",
      "         [2.5060e-09, 6.7625e-06, 2.4697e-03, 1.2207e-01, 8.1651e-01,\n",
      "          7.3916e-01]],\n",
      "\n",
      "        [[1.2317e-04, 1.8292e-02, 3.6764e-01, 1.0000e+00, 3.6812e-01,\n",
      "          1.8340e-02],\n",
      "         [4.5004e-06, 1.8476e-03, 1.0266e-01, 7.7195e-01, 7.8559e-01,\n",
      "          1.0820e-01],\n",
      "         [4.5004e-06, 1.8476e-03, 1.0266e-01, 7.7195e-01, 7.8559e-01,\n",
      "          1.0820e-01],\n",
      "         ...,\n",
      "         [1.0849e-17, 1.0709e-12, 1.4307e-08, 2.5868e-05, 6.3297e-03,\n",
      "          2.0961e-01],\n",
      "         [1.0849e-17, 1.0709e-12, 1.4307e-08, 2.5868e-05, 6.3297e-03,\n",
      "          2.0961e-01],\n",
      "         [1.0849e-17, 1.0709e-12, 1.4307e-08, 2.5868e-05, 6.3297e-03,\n",
      "          2.0961e-01]],\n",
      "\n",
      "        [[2.2497e-05, 5.7456e-03, 1.9859e-01, 9.2898e-01, 5.8811e-01,\n",
      "          5.0387e-02],\n",
      "         [2.2497e-05, 5.7456e-03, 1.9859e-01, 9.2898e-01, 5.8811e-01,\n",
      "          5.0387e-02],\n",
      "         [1.7542e-05, 4.8320e-03, 1.8013e-01, 9.0881e-01, 6.2053e-01,\n",
      "          5.7341e-02],\n",
      "         ...,\n",
      "         [2.4490e-08, 3.8981e-05, 8.3972e-03, 2.4481e-01, 9.6589e-01,\n",
      "          5.1575e-01],\n",
      "         [2.4490e-08, 3.8981e-05, 8.3972e-03, 2.4481e-01, 9.6589e-01,\n",
      "          5.1575e-01],\n",
      "         [1.0849e-17, 1.0709e-12, 1.4307e-08, 2.5868e-05, 6.3297e-03,\n",
      "          2.0961e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[4.1243e-05, 8.7284e-03, 2.4999e-01, 9.6901e-01, 5.0833e-01,\n",
      "          3.6088e-02],\n",
      "         [4.1243e-05, 8.7284e-03, 2.4999e-01, 9.6901e-01, 5.0833e-01,\n",
      "          3.6088e-02],\n",
      "         [3.2637e-05, 7.4319e-03, 2.2903e-01, 9.5522e-01, 5.3917e-01,\n",
      "          4.1186e-02],\n",
      "         ...,\n",
      "         [1.0522e-10, 5.6383e-07, 4.0888e-04, 4.0130e-02, 5.3302e-01,\n",
      "          9.5814e-01],\n",
      "         [1.0849e-17, 1.0709e-12, 1.4307e-08, 2.5868e-05, 6.3297e-03,\n",
      "          2.0961e-01],\n",
      "         [1.0849e-17, 1.0709e-12, 1.4307e-08, 2.5868e-05, 6.3297e-03,\n",
      "          2.0961e-01]],\n",
      "\n",
      "        [[4.5881e-05, 9.3887e-03, 2.6001e-01, 9.7453e-01, 4.9432e-01,\n",
      "          3.3934e-02],\n",
      "         [3.5939e-05, 7.9415e-03, 2.3749e-01, 9.6117e-01, 5.2646e-01,\n",
      "          3.9025e-02],\n",
      "         [3.5939e-05, 7.9415e-03, 2.3749e-01, 9.6117e-01, 5.2646e-01,\n",
      "          3.9025e-02],\n",
      "         ...,\n",
      "         [5.8428e-08, 7.5359e-05, 1.3154e-02, 3.1074e-01, 9.9344e-01,\n",
      "          4.2983e-01],\n",
      "         [5.0260e-08, 6.7255e-05, 1.2180e-02, 2.9851e-01, 9.9014e-01,\n",
      "          4.4447e-01],\n",
      "         [4.6395e-11, 2.9448e-07, 2.5296e-04, 2.9408e-02, 4.6269e-01,\n",
      "          9.8520e-01]],\n",
      "\n",
      "        [[8.9262e-05, 1.4744e-02, 3.2960e-01, 9.9714e-01, 4.0826e-01,\n",
      "          2.2622e-02],\n",
      "         [2.4297e-05, 6.0608e-03, 2.0461e-01, 9.3481e-01, 5.7802e-01,\n",
      "          4.8369e-02],\n",
      "         [2.4297e-05, 6.0608e-03, 2.0461e-01, 9.3481e-01, 5.7802e-01,\n",
      "          4.8369e-02],\n",
      "         ...,\n",
      "         [1.0849e-17, 1.0709e-12, 1.4307e-08, 2.5868e-05, 6.3297e-03,\n",
      "          2.0961e-01],\n",
      "         [1.0849e-17, 1.0709e-12, 1.4307e-08, 2.5868e-05, 6.3297e-03,\n",
      "          2.0961e-01],\n",
      "         [1.0849e-17, 1.0709e-12, 1.4307e-08, 2.5868e-05, 6.3297e-03,\n",
      "          2.0961e-01]]]), tensor([[ 1, 13, 13, 15, 15,  0,  0,  3,  3,  2,  2,  4],\n",
      "        [ 0,  2,  2,  1,  1,  3,  3,  0,  0,  0,  0,  0],\n",
      "        [10, 10, 13,  1,  1,  2,  2,  0,  0, 11, 11,  0],\n",
      "        [15, 18, 18,  3,  3,  0,  0, 11, 11,  1,  1, 10],\n",
      "        [16, 16, 15, 13,  5,  4,  4,  9,  9,  8,  8,  0],\n",
      "        [12, 17, 17, 14,  4,  9,  9,  8,  8,  5,  5, 15],\n",
      "        [19, 19, 17, 17,  7,  6,  6,  9,  9,  8,  8, 14],\n",
      "        [18, 18, 16, 16,  6,  7,  7,  8,  8,  9,  9, 11],\n",
      "        [15, 15, 18, 17,  5,  5,  8,  8,  7,  7,  4,  4],\n",
      "        [19, 14, 14, 16,  5,  5,  9,  9,  4,  4,  7,  7],\n",
      "        [11,  2,  2, 10, 10,  3,  3, 13, 13,  0,  0,  0],\n",
      "        [10, 13, 13, 18, 16, 11, 11,  3,  3,  2,  2,  7],\n",
      "        [ 5, 17, 17, 14, 12, 12,  6,  0,  0,  0,  0,  0],\n",
      "        [11, 11,  0,  0,  4,  2, 16, 16, 13, 13, 15, 10],\n",
      "        [ 9,  9,  5, 19, 19, 12, 14, 14,  6,  0,  0,  0],\n",
      "        [ 8,  8,  3,  4,  0,  0, 18, 18, 15, 15, 13,  7],\n",
      "        [ 4,  4,  9,  7,  7, 11, 13, 13, 16, 16, 18,  0],\n",
      "        [ 5,  5, 12, 12,  8,  6,  6, 17, 17, 19,  0,  0],\n",
      "        [ 8,  7,  7,  3,  3, 11, 15, 15, 18, 18, 16,  0],\n",
      "        [ 9, 14, 14,  6,  6, 19, 19, 17,  0,  0,  0,  0]]), tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]]))\n",
      "2426\n",
      "Unique shapes in SDT_out: {4}\n"
     ]
    }
   ],
   "source": [
    "print(len(SDT_training))\n",
    "print(SDT_training[0])\n",
    "print(len(target_training[['cleavage_energy']]))\n",
    "\n",
    "# Check for consistent shapes\n",
    "shapes = [len(data) for data in SDT_training]\n",
    "unique_shapes = set(shapes)\n",
    "print(f\"Unique shapes in SDT_training: {unique_shapes}\")\n",
    "assert len(unique_shapes) == 1, \"All elements in SDT_training must have the same shape.\"\n",
    "\n",
    "# Ensure that target_training has no missing values\n",
    "assert not target_training['cleavage_energy'].isnull().any(), \"target_training contains missing values.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module because the following parameters were re-set: atom_fea_len, classification, cutoff, h_fea_len, n_conv, n_h, nbr_fea_len, orig_atom_fea_len, use_distance.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dataset does not have consistent lengths.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 43\u001b[0m\n\u001b[0;32m     11\u001b[0m net \u001b[38;5;241m=\u001b[39m MyNet(\n\u001b[0;32m     12\u001b[0m     CrystalGraphConvNet,\n\u001b[0;32m     13\u001b[0m     module__orig_atom_fea_len \u001b[38;5;241m=\u001b[39m orig_atom_fea_len,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[cp, load_best_valid_loss, LR_schedule]\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m net\u001b[38;5;241m.\u001b[39minitialize()\n\u001b[1;32m---> 43\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSDT_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_training\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleavage_energy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\skorch\\regressor.py:82\u001b[0m, in \u001b[0;36mNeuralNetRegressor.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See ``NeuralNet.fit``.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# pylint: disable=useless-super-delegation\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# this is actually a pylint bug:\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# https://github.com/PyCQA/pylint/issues/1085\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNeuralNetRegressor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\skorch\\net.py:1319\u001b[0m, in \u001b[0;36mNeuralNet.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized_:\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\skorch\\net.py:1278\u001b[0m, in \u001b[0;36mNeuralNet.partial_fit\u001b[1;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_train_begin\u001b[39m\u001b[38;5;124m'\u001b[39m, X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my)\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\skorch\\net.py:1190\u001b[0m, in \u001b[0;36mNeuralNet.fit_loop\u001b[1;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m   1188\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_epoch_begin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mon_epoch_kwargs)\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single_epoch(iterator_valid, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1194\u001b[0m                           step_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_step, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m   1196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mon_epoch_kwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\skorch\\net.py:1228\u001b[0m, in \u001b[0;36mNeuralNet.run_single_epoch\u001b[1;34m(self, iterator, training, prefix, step_fn, **fit_params)\u001b[0m\n\u001b[0;32m   1226\u001b[0m step \u001b[38;5;241m=\u001b[39m step_fn(batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mrecord_batch(prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, step[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m-> 1228\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m (\u001b[43mget_len\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m))\n\u001b[0;32m   1229\u001b[0m               \u001b[38;5;28;01melse\u001b[39;00m get_len(batch))\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mrecord_batch(prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size)\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch\u001b[38;5;241m=\u001b[39mbatch, training\u001b[38;5;241m=\u001b[39mtraining, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstep)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\skorch\\dataset.py:82\u001b[0m, in \u001b[0;36mget_len\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     80\u001b[0m len_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(lens)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(len_set) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset does not have consistent lengths.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(len_set)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Dataset does not have consistent lengths."
     ]
    }
   ],
   "source": [
    "#further spilt the training data into train and validate set by 8:2 ratio to avoid overfitting\n",
    "train_test_splitter = ShuffleSplit(test_size=0.2, random_state=42)\n",
    "LR_schedule = LRScheduler()\n",
    "\n",
    "class MyNet(NeuralNetRegressor):\n",
    "    def get_loss(self, y_pred, y_true, **kwargs):\n",
    "        y_pred = y_pred[0] if isinstance(y_pred, tuple) else y_pred  # discard the 2nd output\n",
    "        return super().get_loss(y_pred, y_true, **kwargs)\n",
    "\n",
    "## below is the sigopt best assignment\n",
    "net = MyNet(\n",
    "    CrystalGraphConvNet,\n",
    "    module__orig_atom_fea_len = orig_atom_fea_len,\n",
    "    module__nbr_fea_len = nbr_fea_len,\n",
    "    batch_size=87,  \n",
    "    module__classification=False,\n",
    "    lr=np.exp(-6.465085550816676),     \n",
    "    max_epochs=300,\n",
    "    module__atom_fea_len=43,\n",
    "    module__h_fea_len=114,\n",
    "    module__n_conv=8,\n",
    "    module__n_h=3, \n",
    "    module__use_distance=False,\n",
    "    module__cutoff=100,\n",
    "    optimizer=AdamW,\n",
    "    optimizer__weight_decay=1e-2,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__collate_fn = collate_pool,\n",
    "    iterator_train__shuffle=True, #VERY IMPORTANT\n",
    "    iterator_valid__pin_memory=True,\n",
    "    iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn = collate_pool,\n",
    "    iterator_valid__shuffle=False, #This should be False, which is the default\n",
    "    device=device,\n",
    "    criterion=torch.nn.L1Loss,\n",
    "    dataset=MergeDataset,\n",
    "    train_split = ValidSplit(cv=train_test_splitter),\n",
    "    callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    ")\n",
    "\n",
    "net.initialize()\n",
    "net.fit(SDT_training,np.array(target_training[['cleavage_energy']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions and visualize the predictions with parity plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = {'doc_index': list(target_training['doc_index']),\n",
    "                 'type': 'train', \n",
    "                 'actual_value':np.exp(target_training['cleavage_energy']),\n",
    "                 'predicted_value':np.exp(net.predict(SDT_training).reshape(-1))}\n",
    "\n",
    "test_data = {'doc_index': list(target_test['doc_index']),\n",
    "             'type': 'test',\n",
    "            'actual_value':np.exp(target_test['cleavage_energy']),\n",
    "            'predicted_value':np.exp(net.predict(SDT_test).reshape(-1))}\n",
    "\n",
    "\n",
    "df_training = pd.DataFrame(training_data)\n",
    "df_test = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "ax.scatter(df_training['actual_value'], df_training['predicted_value'], color='yellowgreen', \n",
    "           marker='o', alpha=0.5, label='train: MAE=%0.4f eV/$\\AA^2$, RMSE=%0.3f eV/$\\AA^2$'\\\n",
    "            %(mean_absolute_error(df_training['actual_value'], df_training['predicted_value']), \n",
    "              np.sqrt(mean_squared_error(df_training['actual_value'], df_training['predicted_value']))))\n",
    "\n",
    "ax.scatter(df_test['actual_value'], df_test['predicted_value'], color='cornflowerblue', \n",
    "           marker='o', alpha=0.5, label='test: MAE=%0.4f eV/$\\AA^2$, RMSE=%0.3f eV/$\\AA^2$'\\\n",
    "            %(mean_absolute_error(df_test['actual_value'], df_test['predicted_value']), \n",
    "              np.sqrt(mean_squared_error(df_test['actual_value'], df_test['predicted_value']))))\n",
    "\n",
    "ax.plot([min(df_training['actual_value']), max(df_training['actual_value'])-0.25], \n",
    "        [min(df_training['actual_value']), max(df_training['actual_value'])-0.25], 'k--')\n",
    "\n",
    "# format graph\n",
    "ax.tick_params(labelsize=20)\n",
    "ax.set_xlabel('DFT Energy (eV/$\\AA^2$)', fontsize=20)\n",
    "ax.set_ylabel('CGCNN predicted Energy (eV/$\\AA^2$)', fontsize=20)\n",
    "ax.set_xlim(0,0.35)\n",
    "ax.set_ylim(0,0.35)\n",
    "#ax.set_title('Multi-element ', fontsize=14) \n",
    "ax.legend(fontsize=15, loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get atomic contributions trajectories\n",
    "\n",
    "We picked the ones with reasonably accurate prediction as an example, but you can loop through the test data index and make trajectories of atomic contriution for all test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_idx = np.where(np.array((abs(df_test['actual_value'] - df_test['predicted_value']))) < 0.00005)[0]\n",
    "\n",
    "for idx in visual_idx:\n",
    "    doc_idx = int(df_test.iloc[idx]['doc_index']) \n",
    "    out, atom_fea = net.forward([SDT_list[doc_idx]])\n",
    "    contributions = atom_fea.cpu().data.numpy().reshape(-1)\n",
    "    atoms = mongo.make_atoms_from_doc(docs[doc_idx])\n",
    "    atoms.set_initial_charges(np.exp(contributions))\n",
    "    atoms.write('./Traj/docs_%d.traj'%(doc_idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
